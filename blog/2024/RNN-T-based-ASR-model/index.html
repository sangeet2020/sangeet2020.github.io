<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>RNN-T in Automatic Speech Recognition | Sangeet Sagar</title> <meta name="author" content="Sangeet Sagar"> <meta name="description" content="The article explores the architecture and training processes of the RNN-T model in ASR."> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%92%AC&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://sangeet2020.github.io/blog/2024/RNN-T-based-ASR-model/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "RNN-T in Automatic Speech Recognition",
      "description": "The article explores the architecture and training processes of the RNN-T model in ASR.",
      "published": "January 14, 2024",
      "authors": [
        {
          "author": "Sangeet Sagar",
          "authorURL": "",
          "affiliations": [
            {
              "name": "EML Speech Technology GmbH",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Sangeet </span>Sagar</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">More</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/ms-thesis/">MS Thesis</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/bc-thesis/">BC Thesis</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>RNN-T in Automatic Speech Recognition</h1> <p>The article explores the architecture and training processes of the RNN-T model in ASR.</p> </d-title> <d-byline></d-byline> <d-article> <h2 id="introduction"><strong>Introduction</strong></h2> <p>In this blog post, we will briefly discuss the popular transducer based model- RNN-T in speech recognition. It does not discuss everything from one particular paper but represents my understanding from multiple sources and <a href="https://lorenlugosch.github.io/posts/2020/11/transducer/" rel="external nofollow noopener" target="_blank">Lugosch, 2020</a>’s blog on the same topic. Lately, these models have shown popular interest in the industry due to its natural streaming ability.</p> <h2 id="model-components"><strong>Model Components</strong></h2> <p>The RNN-T model comprises three modules: an encoder, a decoder, and a joiner network. Notably, the predictor in RNN-T operates in an autoregressive manner- meaning the previous output generated by the joiner is fed back into the predictor in order to predict the next output.</p> <p>The joiner network combines the outputs of the encoder and predictor, generating a probability distribution over all labels and a null output (\(\phi\)). Note that the predictor relies only on \(\mathbf{y}\) (labels from the text transcript) rather than both \(\mathbf{x}\) and \(\mathbf{y}\), unlike attention-based models. This means one can pre-train the predictor model on a large amount of text.</p> <h2 id="greedy-search-algorithm"><strong>Greedy Search Algorithm</strong></h2> <p>During the inference phase, RNN-T employs a greedy search algorithm. At each time step \(t\), decisions are made by maximizing computed probabilities. If a label is selected, it is appended to the output sequence \(y\); otherwise, if a blank label \(\phi\) is encountered we move to the next time-step, maintaining the sequential prediction process.</p> <p>Starting with \(t=1\), \(u=0\), and an empty set \(y={}\), the initial output (\(h_{t,u}\) computed using \(f_t\) and \(g_t\)) unfolds with two scenarios:</p> <ol> <li>If \(\arg \max(h_{t,u})\) corresponds to a label (let’s say \([A]\)), then \(y={A}\), and the time step \(t\) remains at 1.</li> <li>If \(\arg \max(h_{t,u})\) is \(\phi\) (indicating a blank label), the predictor advances to the next time step, making \(t=2\), i.e., \(t\) increments by 1.</li> </ol> <div class="row justify-content-center"> <div class="col-sm mt-3 mt-md-0" style="max-width: 600px; text-align: center;"> <figure> <picture> <img src="/assets/img/greedy_search.png" class="img-fluid rounded z-depth-1 mx-auto" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Illustration of a greedy search algorithm in RNN-T. <a href="https://lorenlugosch.github.io/posts/2020/11/transducer/" rel="external nofollow noopener" target="_blank">Reference</a> (Lugosch, 2020) </div> <h2 id="training-dynamics"><strong>Training Dynamics</strong></h2> <p>Training RNN-T involves horizontal transitions for blank labels and vertical transitions for actual labels. The forward variable \(\alpha_{t,u}\) is computed using a recursive formula:</p> \[\alpha_{t,u} = h_{t, u-1} [y_{u-1}] \cdot \alpha_{t, u-1} + h_{t-1, u}[\phi] \cdot\alpha_{t-1, u}\] <p>The above equation represents the computation of the forward variable \(\alpha_{t,u}\). Following this computation, we derive the probability \(p(\mathbf{y}|\mathbf{x})\), indicating the likelihood of a label given acoustic features, expressed as</p> \[p(\mathbf{y}|\mathbf{x}) = \alpha_{T,U} \cdot h_{T,U} [\phi]\] <div class="row justify-content-center"> <div class="col-sm mt-3 mt-md-0" style="max-width: 400px; text-align: center;"> <figure> <picture> <img src="/assets/img/rnnt_forward_message.png" class="img-fluid rounded z-depth-1 mx-auto" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Computation of forward variable in RNN-T training. <a href="https://lorenlugosch.github.io/posts/2020/11/transducer/" rel="external nofollow noopener" target="_blank">Reference</a> (Lugosch, 2020) </div> <h2 id="state-of-the-art-pruned-rnn-t"><strong>State-of-the-Art: Pruned RNN-T</strong></h2> <p>A noteworthy advancement is introduced in the paper titled ‘‘Pruned RNN-T for fast, memory-efficient ASR training’’ (<a href="https://arxiv.org/abs/2206.13236" rel="external nofollow noopener" target="_blank">link</a>). This involves a stateless prediction network, an alternative to the traditional RNN decoder. The predictor, similar to a bi-gram language model, simplifies the architecture by relying solely on the last output symbol, eliminating the need for recurrent layers. Its sole purpose lies in assisting the model to output an actual label or a blank label.</p> <h2 id="alignment-in-rnn-t"><strong>Alignment in RNN-T</strong></h2> <p>The Transducer in RNN-T defines a set of possible monotonic alignments between the input sequence \(x\) and the output sequence \(y\). These alignments, illustrated using an example with the word “CAT,” showcase the model’s ability to align labels with input features.</p> <div class="row justify-content-center"> <div class="col-sm mt-3 mt-md-0" style="max-width: 600px; text-align: center;"> <figure> <picture> <img src="/assets/img/cat-align-1.png" class="img-fluid rounded z-depth-1 mx-auto" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>(This example along with the equation has been taken from <a href="https://lorenlugosch.github.io/posts/2020/11/transducer/" rel="external nofollow noopener" target="_blank">Lugosch, 2020</a>)</p> <p>We can calculate the probability of one of these alignments by multiplying together the values of each edge along the path:</p> \[\mathbf{z} = \phi, C, A, \phi, T, \phi, \phi\] \[p(\mathbf{z} | \mathbf{x}) = h_{1,0}[\phi] \cdot h_{2,0}[C] \cdot h_{2,1}[A] \cdot h_{2,2}[\phi] \cdot h_{3,2}[T] \cdot h_{3,3}[\phi] \cdot h_{4,3}[\phi]\] <h2 id="conclusion"><strong>Conclusion</strong></h2> <p>As we conclude the post, you now possess at least a basic understanding of RNN-T models and the functional principle behind it in the context of Automatic Speech Recognition (ASR). The way encoder, decoder, and joiner networks work together, along with features like stateless predictions, makes RNN-T easier to train.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Sangeet Sagar. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>