<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>SLURM Tutorial | Sangeet Sagar</title> <meta name="author" content="Sangeet Sagar"> <meta name="description" content="SLURM Commands- An Overview of Job Scheduling and Cluster Management System"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%92%AC&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://sangeet2020.github.io/blog/2022/slurm_tutorial/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Sangeet </span>Sagar</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">More</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/ms-thesis/">MS Thesis</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/bc-thesis/">BC Thesis</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">SLURM Tutorial</h1> <p class="post-meta">November 12, 2022</p> <p class="post-tags"> <a href="/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a>   ·   <a href="/blog/category/tutorial"> <i class="fas fa-tag fa-sm"></i> tutorial</a>   </p> </header> <article class="post-content"> <h2 id="introduction">Introduction</h2> <p>SLURM is an open-source cluster management and job scheduling system for Linux Clusters. Main function</p> <ul> <li>Allocates access to resources (computer nodes)</li> <li>Provides a framework to run and monitor jobs on allocated nodes</li> <li>Manage a job queue for competing resource requests.</li> </ul> <h2 id="overview-of-slurm-commands">Overview of SLURM commands</h2> <ul> <li> <code class="language-plaintext highlighter-rouge">sinfo</code> : display compute partition and node information</li> <li> <code class="language-plaintext highlighter-rouge">sbatch</code> : submit a job script for remote execution</li> <li> <code class="language-plaintext highlighter-rouge">srun</code> : launch parallel tasks (job steps) for MPI jobs</li> <li> <code class="language-plaintext highlighter-rouge">salloc</code> : allocate resources for an interactive job</li> <li> <code class="language-plaintext highlighter-rouge">squeue</code> : display status for jobs and job steps</li> <li> <code class="language-plaintext highlighter-rouge">sprio</code> : display job priority information</li> <li> <code class="language-plaintext highlighter-rouge">scancel</code> : cancel pending or running jobs</li> <li> <code class="language-plaintext highlighter-rouge">sstat</code> : display status information for running jobs</li> <li> <code class="language-plaintext highlighter-rouge">sacct</code> : display accounting information for past jobs</li> <li> <code class="language-plaintext highlighter-rouge">seff</code> : display job efficiency information for past jobs</li> <li> <code class="language-plaintext highlighter-rouge">scontrol</code> : display or modify slurm configuration and state</li> </ul> <h2 id="codes-for-common-node-state">Codes for common node state</h2> <ul> <li> <code class="language-plaintext highlighter-rouge">Draining</code> : the node is currently executing a job, but will not be allocated additional jobs</li> <li> <code class="language-plaintext highlighter-rouge">Maint</code>: The node is currently in a reservation with a flag value of maintenance</li> <li> <code class="language-plaintext highlighter-rouge">Mixed</code>: The node has some of its CPUs allocated with others are IDLE</li> <li> <code class="language-plaintext highlighter-rouge">Reserved</code>: The node is in advanced and not generally available.</li> </ul> <p><code class="language-plaintext highlighter-rouge">myaccount</code> : to know your default project account</p> <h2 id="view-the-status-of-all-running-jobs">View the status of all running jobs</h2> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>squeue -u sagar -o "%.6i %9P %15j %.6u %.10T %.10M %.10l %.6D %10R %s %.10g"
</code></pre></div></div> <h2 id="interactive-job">Interactive job</h2> <p>This basically helps you run your script interactively by allocating a terminal/console for you and you get to see all verbose generated by your script.</p> <h3 id="interactive-cpu--job">Interactive CPU job</h3> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>srun --job-name "InteractiveJob" --cpus-per-task 8 --ptybash
</code></pre></div></div> <p>here</p> <ul> <li> <code class="language-plaintext highlighter-rouge">InteractiveJob</code> : name of your job. (when you run <code class="language-plaintext highlighter-rouge">squeue -u sagar</code> you can identify your job with that name)</li> <li> <code class="language-plaintext highlighter-rouge">--cpus-per-task 8</code> : you need 8 cores for this job (16 threads)</li> <li> <code class="language-plaintext highlighter-rouge">--ptybash</code> : this will open a bash terminal for you and let YOU run the job (that’s why we call it an interactive job, since you can interact with the terminal, and control the arguments for your script)</li> </ul> <p>You can also specify other parameters like</p> <ul> <li> <code class="language-plaintext highlighter-rouge">--mem-per-cpu=16gb</code> : each core to have 16 Gb RAM</li> <li> <code class="language-plaintext highlighter-rouge">--time=30:00:00</code> : Run this task no longer than 30 hrs.</li> </ul> <h3 id="interactive-gpu-job">Interactive GPU job</h3> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>srun \
-n 1 \
--container-mounts=/netscratch/$USER:/netscratch/$USER,/ds:/ds:ro,"`pwd`":"`pwd`",/home/$USER/:/home/$USER/ \
--container-image=/netscratch/sagar/docker_images/images/10.2.sqsh \
--container-workdir="`pwd`" \
--pty \
--cpus-per-gpu=8 \
--gpus-per-task=1 \
--partition=RTXA6000,A100,V100-32GB,RTX3090 \
/bin/bash
</code></pre></div></div> <p>change these parameters as per your requirement</p> <ul> <li><code class="language-plaintext highlighter-rouge">container-image</code></li> <li><code class="language-plaintext highlighter-rouge">cpus-per-gpu</code></li> <li><code class="language-plaintext highlighter-rouge">gpus-per-task</code></li> <li><code class="language-plaintext highlighter-rouge">partition</code></li> </ul> <h2 id="job-submission-gpu">Job submission (GPU)</h2> <h3 id="choice-1--short-way">Choice 1- short way</h3> <p>Create a script with a name, let’s say <code class="language-plaintext highlighter-rouge">sbatch_train.sh</code></p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/bin/bash</span>

<span class="c">#SBATCH --nodes=1               # Number of nodes or servers. See: http://koeln.kl.dfki.de:3000/d/slurm-resources/resources?orgId=1&amp;refresh=15s</span>
<span class="c">#SBATCH --ntasks-per-node=1     # Number of task in each node, we want 1 </span>
<span class="c">#SBATCH --cpus-per-task=4       # We want 4 cores for this job.</span>
<span class="c">#SBATCH --mem-per-cpu=16gb      # each core to have 16 Gb RAM</span>
<span class="c">#SBATCH --gres=gpu:4            # We want 4 GPUs in each node for this job.</span>
<span class="c">#SBATCH --time=30:00:00         # Run this task no longer that 30 hrs.</span>
<span class="c">#SBATCH --partition=RTXA6000,V100-32GB,RTX3090  # Run this only in these mentioned GPUs. If you dont have any choice over GPUs, remove this parameter.</span>
<span class="c">#SBATCH --job-name=mimic_loss</span>
<span class="c">#SBATCH --output=mimic_loss_%A.logs</span>

srun <span class="nt">-K</span> <span class="se">\</span>
<span class="nt">--container-mounts</span><span class="o">=</span>/netscratch/<span class="nv">$USER</span>:/netscratch/<span class="nv">$USER</span>,/ds:/ds:ro,<span class="s2">"</span><span class="sb">`</span><span class="nb">pwd</span><span class="sb">`</span><span class="s2">"</span>:<span class="s2">"</span><span class="sb">`</span><span class="nb">pwd</span><span class="sb">`</span><span class="s2">"</span>,/home/<span class="nv">$USER</span>/:/home/<span class="nv">$USER</span>/ <span class="se">\</span>
<span class="nt">--container-image</span><span class="o">=</span>/netscratch/sagar/docker_images/images/10.2.sqsh <span class="se">\</span>
<span class="nt">--container-workdir</span><span class="o">=</span><span class="s2">"</span><span class="sb">`</span><span class="nb">pwd</span><span class="sb">`</span><span class="s2">"</span> <span class="se">\</span>
python train.py <span class="nt">--output_dir</span> output
</code></pre></div></div> <p>where <code class="language-plaintext highlighter-rouge">--container-image</code> should point to your docker-image that has all packages installed like a python version of your choice, PyTorch or TensorFlow.<br> <strong>Submit your job using</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sbatch sbatch_train.sh
</code></pre></div></div> <h3 id="choice-2--long-way">Choice 2- long way</h3> <p>I personally prefer using a docker image with minimal installation i.e. supported with just <code class="language-plaintext highlighter-rouge">cuda</code> and <code class="language-plaintext highlighter-rouge">cudnn</code> version of my choice. I rather use python, pytorch, tensorflow etc. (all required libraries/packages) installed locally in my conda virtual env or python virtual env. (You can also download the docker image of your choice: <code class="language-plaintext highlighter-rouge">https://hub.docker.com/r/nvidia/cuda/tags</code>)</p> <ol> <li>Create a script with a name, let’s say <code class="language-plaintext highlighter-rouge">sbatch_train.sh</code> </li> </ol> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/bin/bash</span>

<span class="c">#SBATCH --nodes=1               # Number of nodes or servers. See: http://koeln.kl.dfki.de:3000/d/slurm-resources/resources?orgId=1&amp;refresh=15s</span>
<span class="c">#SBATCH --ntasks-per-node=1     # Number of task in each node, we want 1 </span>
<span class="c">#SBATCH --cpus-per-task=4       # We want 4 cores for this job.</span>
<span class="c">#SBATCH --mem-per-cpu=16gb      # each core to have 16 Gb RAM</span>
<span class="c">#SBATCH --gres=gpu:4            # We want 4 GPUs in each node for this job.</span>
<span class="c">#SBATCH --time=30:00:00         # Run this task no longer than 30 hrs.</span>
<span class="c">#SBATCH --partition=RTXA6000,V100-32GB,RTX3090  # Run this only in these mentioned GPUs. If you don't have any choice over GPUs, remove this parameter.</span>
<span class="c">#SBATCH --job-name=mimic_loss</span>
<span class="c">#SBATCH --output=mimic_loss_%A.logs</span>

<span class="nb">echo</span> <span class="s2">"#############################"</span>
<span class="nb">date
echo</span> <span class="s2">"Current dir: "</span> <span class="k">${</span><span class="nv">SLURM_SUBMIT_DIR</span><span class="k">}</span>
<span class="nb">echo</span> <span class="s2">"Hostname: </span><span class="sb">`</span><span class="nb">hostname</span><span class="sb">`</span><span class="s2">"</span>

<span class="c"># Print the task details.</span>
<span class="nb">echo</span> <span class="s2">"Job ID: </span><span class="k">${</span><span class="nv">SLURM_JOBID</span><span class="k">}</span><span class="s2">"</span>
<span class="nb">echo</span> <span class="s2">"SLURM array task ID:  </span><span class="k">${</span><span class="nv">SLURM_ARRAY_TASK_ID</span><span class="k">}</span><span class="s2">"</span>
<span class="nb">echo</span> <span class="s2">"Node list: </span><span class="k">${</span><span class="nv">SLURM_JOB_NODELIST</span><span class="k">}</span><span class="s2">"</span> 
<span class="nb">echo</span> <span class="s2">"Cluster name: </span><span class="k">${</span><span class="nv">SLURM_CLUSTER_NAME</span><span class="k">}</span><span class="s2">"</span>
<span class="nb">echo</span> <span class="s2">"Partition name: </span><span class="k">${</span><span class="nv">SLURM_JOB_PARTITION</span><span class="k">}</span><span class="s2">"</span> 
<span class="nb">echo</span> <span class="s2">"Using: </span><span class="sb">`</span>which python<span class="sb">`</span><span class="s2">"</span>
<span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"#############################</span><span class="se">\n</span><span class="s2">"</span>

srun <span class="nt">-K</span> <span class="se">\</span>
<span class="nt">--container-mounts</span><span class="o">=</span>/netscratch/<span class="nv">$USER</span>:/netscratch/<span class="nv">$USER</span>,/ds:/ds:ro,<span class="s2">"</span><span class="sb">`</span><span class="nb">pwd</span><span class="sb">`</span><span class="s2">"</span>:<span class="s2">"</span><span class="sb">`</span><span class="nb">pwd</span><span class="sb">`</span><span class="s2">"</span>,/home/<span class="nv">$USER</span>/:/home/<span class="nv">$USER</span>/ <span class="se">\</span>
<span class="nt">--container-image</span><span class="o">=</span>/netscratch/sagar/docker_images/images/10.2.sqsh <span class="se">\</span>
<span class="nt">--container-workdir</span><span class="o">=</span><span class="s2">"</span><span class="sb">`</span><span class="nb">pwd</span><span class="sb">`</span><span class="s2">"</span> <span class="se">\</span>
bash run_task.sh
</code></pre></div></div> <p>In my script, I use a basic docker image with minimal installation of just cuda and cudnn (a version of my choice). And to use the locally installed libraries and packages, I activate the virtual environment. All these are done in <code class="language-plaintext highlighter-rouge">run_task.sh</code> script</p> <ol> <li>Your <code class="language-plaintext highlighter-rouge">run_task.sh</code> script will have the actual training script/commands</li> </ol> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/bin/bash</span>

<span class="c"># We jump into the submission dir</span>
<span class="nb">cd</span> <span class="k">${</span><span class="nv">SLURM_SUBMIT_DIR</span><span class="k">}</span>

<span class="c"># Activate the python virtual env</span>
<span class="nb">source</span> /netscratch/sagar/thesis/sb_env/bin/activate

<span class="c"># But if you are using conda (uncomment the lines below)</span>
<span class="c"># . /netscratch/sagar/miniconda/etc/profile.d/conda.sh</span>
<span class="c"># conda activate my_env_name</span>

<span class="c"># If you need other packages that are not available in pip/conda, </span>
<span class="c"># you can install them this way</span>
apt-get update
apt-get <span class="nt">-y</span> <span class="nb">install </span>libsndfile1


<span class="c">## Single-GPU training</span>
<span class="c"># Execute this python script (this will use 1 GPU regardless you have taken 4 or 8 GPUs)</span>
python train.py hparams/robust_asr.yaml

<span class="c">## Multi GPU training</span>
<span class="c"># If you want your script to use all GPUs that you requested, follow this </span>
<span class="nv">NUM_GPUS</span><span class="o">=</span>4
python <span class="nt">-m</span> torch.distributed.launch <span class="se">\</span>
<span class="nt">--nproc_per_node</span><span class="o">=</span><span class="k">${</span><span class="nv">NUM_GPUS</span><span class="k">}</span> <span class="se">\</span>
<span class="nt">--nnodes</span><span class="o">=</span><span class="k">${</span><span class="nv">SLURM_JOB_NUM_NODES</span><span class="k">}</span> <span class="se">\</span>
<span class="nt">--node_rank</span><span class="o">=</span><span class="k">${</span><span class="nv">SLURM_NODEID</span><span class="k">}</span> <span class="se">\</span>
train.py hparams/robust_asr.yaml <span class="se">\</span>
<span class="nt">--distributed_launch</span>

<span class="c">## Note</span>
<span class="c"># Multi-gpu jobs will be successful only if your script is adapted to perform DP or DDP training</span>
</code></pre></div></div> <p><strong>Submit your job using</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sbatch sbatch_train.sh
</code></pre></div></div> <h2 id="multi-node-multi-gpu-ddp-training">Multi-node multi-GPU DDP training</h2> <p>To utilize multiple nodes and all GPUs within those nodes, it is important to ensure that all nodes are connected to a master node. This will allow for efficient distribution of work across all available resources. This can be done using</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">MASTER</span><span class="o">=</span><span class="sb">`</span><span class="nb">echo</span> <span class="nv">$SLURM_JOB_NODELIST</span> | <span class="nb">cut</span> <span class="nt">-d</span><span class="s2">","</span> <span class="nt">-f1</span> | <span class="nb">sed</span> <span class="s1">'s/[][]//g'</span> | <span class="nb">cut</span> <span class="nt">-d</span> <span class="s2">"-"</span> <span class="nt">-f</span> 1,2<span class="sb">`</span>
</code></pre></div></div> <p>and then in the job submission script modify parameters this way</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python <span class="nt">-m</span> torch.distributed.launch 
<span class="nt">--nproc_per_node</span><span class="o">=</span><span class="k">${</span><span class="nv">NUM_GPUS</span><span class="k">}</span> 
<span class="nt">--nnodes</span><span class="o">=</span><span class="k">${</span><span class="nv">SLURM_JOB_NUM_NODES</span><span class="k">}</span> 
<span class="nt">--master_addr</span><span class="o">=</span><span class="k">${</span><span class="nv">MASTER</span><span class="k">}</span> 
<span class="nt">--master_port</span><span class="o">=</span>5557 
<span class="nt">--node_rank</span><span class="o">=</span><span class="k">${</span><span class="nv">SLURM_NODEID</span><span class="k">}</span> 
<span class="k">${</span><span class="nv">training_command</span><span class="k">}</span> 
<span class="nt">--distributed_launch</span>
</code></pre></div></div> <h2 id="downloading-a-docker-image">Downloading a docker image</h2> <p>Prepare a script with name <code class="language-plaintext highlighter-rouge">download_docker_img.sh</code></p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">docker_img</span><span class="o">=</span><span class="nv">$1</span>
<span class="nv">docker_img_name</span><span class="o">=</span><span class="nv">$2</span>
srun enroot import <span class="nt">-o</span> /netscratch/<span class="nv">$USER</span>/docker_images/<span class="nv">$docker_img_name</span> docker://<span class="nv">$docker_img</span>
</code></pre></div></div> <p>Download docker image and prepare the sqsh file, E.g.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>./download_docker_img.sh nvidia/pytorch:20.10-py3 pytorch:20.10-py3.sqsh
</code></pre></div></div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/mybashrc/">My Shell Configuration Files</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/install_without_sudo/">Installation without sudo</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/Pruned_RNNT/">Pruned RNN-T Explained</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/icefall_data_prepration/">icefall- Data Prepration Pipeline</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/Gdrive/">CLI-based Google Drive Backup.</a> </li> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Sangeet Sagar. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script defer src="/assets/js/copy_code.js" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>